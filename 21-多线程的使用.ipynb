{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "# 多线程的使用：多线程爬取糗事百科中的段子，保存下来 解析\n",
    "\n",
    "# 爬虫类\n",
    "class CrawlInfo(Thread):\n",
    "    # 初始化\n",
    "    def __init__(self, url_queue, html_queue):\n",
    "        Thread.__init__(self)\n",
    "        self.url_queue = url_queue\n",
    "        self.html_queue = html_queue\n",
    "\n",
    "    def run(self):\n",
    "        headers = {\n",
    "            \"User-Agent\": UserAgent().chrome\n",
    "        }\n",
    "        while self.url_queue.empty() == False:\n",
    "            response = requests.get(self.url_queue.get(), headers=headers)\n",
    "            # print(response.text)\n",
    "            if response.status_code == 200:\n",
    "                # print(response.text)\n",
    "                self.html_queue.put(response.text) # 存储内容到保存queue中\n",
    "\n",
    "# 解析类\n",
    "class ParseInfo(Thread):\n",
    "    def __init__(self, html_queue):\n",
    "        Thread.__init__(self)\n",
    "        self.html_queue = html_queue\n",
    "\n",
    "    def run(self):\n",
    "        while self.html_queue.empty() == False:\n",
    "            # 解析lxml 爬取返回的内容\n",
    "            e = etree.HTML(self.html_queue.get())\n",
    "            # 有25个span\n",
    "            span_content = e.xpath('//div[@class=\"content\"]/span[1]')\n",
    "            # print(span_content)\n",
    "            # 写入文件中\n",
    "            with open('21duanzi.txt','a',encoding='utf-8') as f:\n",
    "                for span in span_content:\n",
    "                    # span内容格式化\n",
    "                    info = span.xpath('string(.)') # . 当前节点\n",
    "                    print(info)\n",
    "                    f.write(info + \"\\n\")  # 每个段子换行\n",
    "\n",
    "# 模拟入口\n",
    "if __name__ == '__main__':\n",
    "    # 存储url的容器\n",
    "    url_queue = Queue()\n",
    "    # 创建存储爬取回来内容的容器\n",
    "    html_queue = Queue()\n",
    "    base_url = 'https://www.qiushibaike.com/text/page/{}/'\n",
    "    for i in range(1, 14):\n",
    "        new_url = base_url.format(i)\n",
    "        url_queue.put(new_url)\n",
    "     # 创建爬虫\n",
    "    crawl_list = []\n",
    "    for i in range(0, 3):\n",
    "        crawl1 = CrawlInfo(url_queue, html_queue)\n",
    "        crawl_list.append(crawl1)\n",
    "        crawl1.start()\n",
    "    # 等待线程结束。 原因：因为主线程不会等待其他线程，没有这个没有数据的原因就是，副线程没有执行完，主线程就结束了\n",
    "    # join([time]): 等待至线程中止。这阻塞调用线程直至线程的join() 方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生\n",
    "    for crawl in crawl_list:\n",
    "        crawl.join()\n",
    "\n",
    "    # 创建解析线程\n",
    "    parse_list = []\n",
    "    for i in range(0, 3):\n",
    "        parse = ParseInfo(html_queue)\n",
    "        parse_list.append(parse)\n",
    "        parse.start()\n",
    "    for parse in parse_list:\n",
    "        parse.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
